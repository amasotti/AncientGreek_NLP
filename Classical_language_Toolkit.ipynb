{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of CLTK\n",
    "\n",
    "Cltk is a MIT-Package to deal with ancient languages. It supports classes for many old languages (Akkadian, Old Russian, Latin, Greek, Sanskrit ....). Some of them are trivial functions (unicode rendering etc...) but others are really nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(txt,lines = False):\n",
    "\n",
    "    '''\n",
    "    DOC: \n",
    "    Loads text from txt files\n",
    "    params: \n",
    "    \t- txt (str) : path of the file\n",
    "    \t- lines (bool) : if True readlines(), else read()\n",
    "\n",
    "    return:\n",
    "    \t- data (str) : the text read off the file\n",
    "    '''\n",
    "    if lines:\n",
    "        with open(txt, \"r\",encoding = \"UTF-8\") as source:\n",
    "            data = source.readlines()\n",
    "    else:\n",
    "        with open(txt, \"r\",encoding = \"UTF-8\") as source:\n",
    "            data = source.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "homerText = load(\"HomerGesamt.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_greek(txt):\n",
    "    txt = re.sub(r\"\\n\", \" \",txt, re.MULTILINE) # delete row break\n",
    "    txt = re.sub(r\"\\\\n\", \" \",txt, re.MULTILINE) # delete row break\n",
    "    txt = re.sub(\"n\", \"\",txt, re.MULTILINE) # delete row break\n",
    "    txt = re.sub(r\";\",\"?\",txt,re.MULTILINE) # change question mark\n",
    "    txt = re.sub(r\"(\\d+(\\.\\d+)?)\",\"\",txt,0,re.MULTILINE) # delete verse number\n",
    "    txt = re.sub(r\"\\w(·)\",\"\",txt,0,re.MULTILINE)\n",
    "    txt = re.sub(r'(\\.|,|;|!|\\\")',\"\",txt,re.MULTILINE)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "homerText = clean_greek(homerText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'μῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος οὐλομένην ἣ μυρίʼ Ἀχαιοῖς ἄλγεʼ ἔθηκε πολλὰς δʼ ἰφθίμους ψυχὰς Ἄϊδι προΐαψεν ἡρώων αὐτοὺς δὲ ἑλώρια τεῦχε κύνεσσιν  '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homerText[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some tokens from homer:\n",
    "some_tokens = homerText[:150].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greek Accentuation & Syllabification\n",
    "\n",
    "This is a separate module, created by James Tauber,which  deals with greek Accentuation and Syllabification, allowing analyzing word accents, dividing word into syllables and printing possible acc patterns for unaccented words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greek_accentuation.syllabify import syllabify, display_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['μῆ', 'νιν']\n",
      "['ἄ', 'ει', 'δε']\n",
      "['θε', 'ὰ']\n",
      "['Πη', 'λη', 'ϊ', 'ά', 'δε', 'ω']\n",
      "['Ἀ', 'χι', 'λῆ', 'ος']\n",
      "['οὐ', 'λο', 'μέ', 'νην']\n",
      "['ἣ']\n",
      "['μυ', 'ρίʼ']\n",
      "['Ἀ', 'χαι', 'οῖς']\n",
      "['ἄλ', 'γεʼ']\n",
      "['ἔ', 'θη', 'κε']\n",
      "['πολ', 'λὰς']\n",
      "['δʼ']\n",
      "['ἰ', 'φθί', 'μους']\n",
      "['ψυ', 'χὰς']\n",
      "['Ἄ', 'ϊ', 'δι']\n",
      "['προ', 'ΐ', 'α', 'ψεν']\n",
      "['ἡ', 'ρώ', 'ων']\n",
      "['αὐ', 'τοὺς']\n",
      "['δὲ']\n",
      "['ἑ', 'λώ', 'ρι', 'α']\n",
      "['τεῦ', 'χε']\n",
      "['κύ', 'νεσ', 'σιν']\n"
     ]
    }
   ],
   "source": [
    "for w in some_tokens:\n",
    "    w_sill = syllabify(w)\n",
    "    print(w_sill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μῆ.νιν\n",
      "ἄ.ει.δε\n",
      "θε.ὰ\n",
      "Πη.λη.ϊ.ά.δε.ω\n",
      "Ἀ.χι.λῆ.ος\n",
      "οὐ.λο.μέ.νην\n",
      "ἣ\n",
      "μυ.ρίʼ\n",
      "Ἀ.χαι.οῖς\n",
      "ἄλ.γεʼ\n",
      "ἔ.θη.κε\n",
      "πολ.λὰς\n",
      "δʼ\n",
      "ἰ.φθί.μους\n",
      "ψυ.χὰς\n",
      "Ἄ.ϊ.δι\n",
      "προ.ΐ.α.ψεν\n",
      "ἡ.ρώ.ων\n",
      "αὐ.τοὺς\n",
      "δὲ\n",
      "ἑ.λώ.ρι.α\n",
      "τεῦ.χε\n",
      "κύ.νεσ.σιν\n"
     ]
    }
   ],
   "source": [
    "for w in some_tokens:\n",
    "    w_sill = display_word(syllabify(w))\n",
    "    print(w_sill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greek_accentuation.syllabify import onset_nucleus_coda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('̓', 'ου', 'λομένην')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onset_nucleus_coda('οὐλομένην')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greek_accentuation.syllabify import syllable_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'A']\n",
      "['A', 'H', 'L']\n",
      "['L', 'A']\n",
      "['H', 'H', 'A', 'A', 'L', 'H']\n",
      "['A', 'A', 'H', 'L']\n",
      "['H', 'L', 'L', 'H']\n",
      "['H']\n",
      "['A', 'A']\n",
      "['A', 'A', 'H']\n",
      "['A', 'L']\n",
      "['L', 'H', 'L']\n",
      "['L', 'A']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-4378e184aa83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msill_parsing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msillaba\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw_sill\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0msyllable_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msillaba\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msyllable_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'τεῦ'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0msill_parsing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"H\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0msyllable_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msillaba\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msyllable_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'θε'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\greek_accentuation\\syllabify.py\u001b[0m in \u001b[0;36msyllable_length\u001b[1;34m(s, final)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnucleus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfinal\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "for w in some_tokens:\n",
    "    w_sill = syllabify(w)\n",
    "    sill_parsing = []\n",
    "    for sillaba in w_sill:\n",
    "        if syllable_length(sillaba) == syllable_length('τεῦ'): # long\n",
    "            sill_parsing.append(\"H\")\n",
    "        elif syllable_length(sillaba) == syllable_length('θε'): #kurz\n",
    "            sill_parsing.append(\"L\")\n",
    "        else:\n",
    "            sill_parsing.append('A') # ancipite\n",
    "    print(sill_parsing)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accentuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greek_accentuation.accentuation import get_accent_type, display_accent_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μῆνιν : AK: properispomenon\n",
      "ἄειδε : AK: proparoxytone\n",
      "θεὰ not classifiable\n",
      "Πηληϊάδεω : AK: proparoxytone\n",
      "Ἀχιλῆος : AK: properispomenon\n",
      "οὐλομένην : AK: paroxytone\n",
      "ἣ not classifiable\n",
      "μυρίʼ : AK: oxytone\n",
      "Ἀχαιοῖς : AK: perispomenon\n",
      "ἄλγεʼ : AK: paroxytone\n",
      "ἔθηκε : AK: proparoxytone\n",
      "πολλὰς not classifiable\n",
      "δʼ not classifiable\n",
      "ἰφθίμους : AK: paroxytone\n",
      "ψυχὰς not classifiable\n",
      "Ἄϊδι : AK: proparoxytone\n",
      "προΐαψεν : AK: proparoxytone\n",
      "ἡρώων : AK: paroxytone\n",
      "αὐτοὺς not classifiable\n",
      "δὲ not classifiable\n",
      "ἑλώρια : AK: proparoxytone\n",
      "τεῦχε : AK: properispomenon\n",
      "κύνεσσιν : AK: proparoxytone\n"
     ]
    }
   ],
   "source": [
    "for w in some_tokens:\n",
    "    try:\n",
    "        print(f\"{w} : AK: {display_accent_type(get_accent_type(w))}\")\n",
    "    except:\n",
    "        print(f\"{w} not classifiable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible accentuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greek_accentuation.accentuation import possible_accentuations, add_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['μῆνίν', 'μῆνῖν', 'μῆ́νιν', 'μῆ͂νιν']\n",
      "['ἄειδέ', 'ἄεῖδε', 'ά̓́ειδε']\n",
      "['θεὰ́', 'θεὰ͂', 'θέὰ']\n",
      "['Πηληϊάδεώ', 'Πηληϊάδεῶ', 'Πηληϊάδέω']\n",
      "['Ἀχιλῆός', 'Ἀχιλῆ͂ος', 'Ἀχίλῆος']\n",
      "['οὐλομένήν', 'οὐλομένῆν', 'οὐλομέ́νην']\n",
      "['ὴ̔́', 'ὴ̔͂']\n",
      "['μυρί́ʼ', 'μυρί͂ʼ', 'μύρίʼ', 'μῦρίʼ']\n",
      "['Ἀχαιοῖ́ς', 'Ἀχαιοῖ͂ς', 'Ἀχαίοῖς']\n",
      "['ἄλγέʼ', 'ά̓́λγεʼ', 'ά̓͂λγεʼ']\n",
      "['ἔθηκέ', 'ἔθῆκε', 'έ̓́θηκε']\n",
      "['πολλὰ́ς', 'πολλὰ͂ς', 'πόλλὰς']\n",
      "Don't know how to deal with δʼ\n",
      "['ἰφθίμούς', 'ἰφθίμοῦς', 'ἰφθί́μους']\n",
      "['ψυχὰ́ς', 'ψυχὰ͂ς', 'ψύχὰς', 'ψῦχὰς']\n",
      "['Ἄϊδί', 'Ἄϊδῖ', 'Ἄΐδι', 'Ἄῗδι', 'Ά̓́ϊδι']\n",
      "['προΐαψέν', 'προΐάψεν', 'προΐᾶψεν', 'προΐ́αψεν']\n",
      "['ἡρώών', 'ἡρώῶν', 'ἡρώ́ων']\n",
      "['αὐτοὺ́ς', 'αὐτοὺ͂ς', 'αὔτοὺς']\n",
      "['δὲ́']\n",
      "['ἑλώριά', 'ἑλώριᾶ', 'ἑλώρία', 'ἑλώρῖα', 'ἑλώ́ρια']\n",
      "['τεῦχέ', 'τεῦ͂χε']\n",
      "['κύνεσσίν', 'κύνεσσῖν', 'κύνέσσιν', 'κύ́νεσσιν']\n"
     ]
    }
   ],
   "source": [
    "for w in some_tokens:\n",
    "    try:\n",
    "        s = syllabify(w)\n",
    "        possible_patterns = []\n",
    "        for ak in possible_accentuations(s):\n",
    "            possible_patterns.append(str(add_accent(s,ak)))\n",
    "        print(possible_patterns)\n",
    "    except:\n",
    "        print(f\"Don't know how to deal with {w}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'εἴσηλθον'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from greek_accentuation.accentuation import recessive\n",
    "recessive('εἰσηλθον')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLTK with TLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GIT_PYTHON_REFRESH'] = \"quiet\" # since git is not used directly but only as dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_importer = CorpusImporter('greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greek_software_tlgu',\n",
       " 'greek_text_perseus',\n",
       " 'phi7',\n",
       " 'tlg',\n",
       " 'greek_proper_names_cltk',\n",
       " 'greek_models_cltk',\n",
       " 'greek_treebank_perseus',\n",
       " 'greek_treebank_gorman',\n",
       " 'greek_lexica_perseus',\n",
       " 'greek_training_set_sentence_cltk',\n",
       " 'greek_word2vec_cltk',\n",
       " 'greek_text_lacus_curtius',\n",
       " 'greek_text_first1kgreek',\n",
       " 'greek_text_tesserae']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_importer.list_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latin_text_perseus',\n",
       " 'latin_treebank_perseus',\n",
       " 'latin_text_latin_library',\n",
       " 'phi5',\n",
       " 'phi7',\n",
       " 'latin_proper_names_cltk',\n",
       " 'latin_models_cltk',\n",
       " 'latin_pos_lemmata_cltk',\n",
       " 'latin_treebank_index_thomisticus',\n",
       " 'latin_lexica_perseus',\n",
       " 'latin_training_set_sentence_cltk',\n",
       " 'latin_word2vec_cltk',\n",
       " 'latin_text_antique_digiliblt',\n",
       " 'latin_text_corpus_grammaticorum_latinorum',\n",
       " 'latin_text_poeti_ditalia',\n",
       " 'latin_text_tesserae']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latin_corpora = CorpusImporter('latin')\n",
    "latin_corpora.list_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from cltk.corpus.utils.formatter import cltk_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = homerText[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = cltk_normalize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = LemmaReplacer(\"greek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['μῆνιν/μῆνις',\n",
       " 'ἄειδε/ἀείδω',\n",
       " 'θεὰ/θεὰ',\n",
       " 'Πηληϊάδεω/Πηληϊάδεω',\n",
       " 'Ἀχιλῆος/ἀχιλλεύς',\n",
       " 'οὐλομένην/οὐλόμενος',\n",
       " 'ἣ/ἣ',\n",
       " 'μυρίʼ/μυρίʼ',\n",
       " 'Ἀχαιοῖς/ἀχαιός',\n",
       " 'ἄλγεʼ/ἄλγεʼ',\n",
       " 'ἔθηκε/τίθημι',\n",
       " 'πολλὰς/πολλὰς',\n",
       " 'δʼ/δʼ',\n",
       " 'ἰφθίμους/ἴφθιμος',\n",
       " 'ψυχὰς/ψυχὰς',\n",
       " 'Ἄϊδι/Ἄϊδι',\n",
       " 'προΐαψεν/προΐαψεν',\n",
       " 'ἡρώων/ἥρως',\n",
       " 'αὐτοὺς/αὐτοὺς',\n",
       " 'δὲ/δὲ',\n",
       " 'ἑλώρια/ἑλώριον',\n",
       " 'τεῦχε/τεύχω',\n",
       " 'κύνεσσιν/κύων']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(sentence,return_raw = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.lemmatize.greek.backoff import BackoffGreekLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = BackoffGreekLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('μῆνιν', 'μῆνις'),\n",
       " ('ἄειδε', 'ἀείδω'),\n",
       " ('θεὰ', 'θεά'),\n",
       " ('Πηληϊάδεω', 'Πηληϊάδεω'),\n",
       " ('Ἀχιλῆος', 'Ἀχιλλεύς'),\n",
       " ('οὐλομένην', 'οὐλόμενος'),\n",
       " ('ἣ', 'ὁ'),\n",
       " ('μυρίʼ', 'μυρίʼ'),\n",
       " ('Ἀχαιοῖς', 'Ἀχαιός'),\n",
       " ('ἄλγεʼ', 'ἄλγεʼ'),\n",
       " ('ἔθηκε', 'τίθημι'),\n",
       " ('πολλὰς', 'πολύς'),\n",
       " ('δʼ', 'δʼ'),\n",
       " ('ἰφθίμους', 'ἴφθιμος'),\n",
       " ('ψυχὰς', 'ψυχή'),\n",
       " ('Ἄϊδι', 'Ἀΐδης'),\n",
       " ('προΐαψεν', 'προΐαψεν'),\n",
       " ('ἡρώων', 'ἥρως'),\n",
       " ('αὐτοὺς', 'αὐτός'),\n",
       " ('δὲ', 'δέ'),\n",
       " ('ἑλώρια', 'ἑλώριον'),\n",
       " ('τεῦχε', 'τεύχω'),\n",
       " ('κύνεσσιν', 'κύων')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(homerText[:150].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos Tagging for Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tag.pos import POSTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = POSTag('greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('μῆνιν', 'N-S---FA-'),\n",
       " ('ἄειδε', 'V2SPMA---'),\n",
       " ('θεὰ', 'N-S---FV-'),\n",
       " ('Πηληϊάδεω', None),\n",
       " ('Ἀχιλῆος', None),\n",
       " ('οὐλομένην', 'A-S---FA-'),\n",
       " ('ἣ', 'P-S---FN-'),\n",
       " ('μυρίʼ', None),\n",
       " ('Ἀχαιοῖς', None),\n",
       " ('ἄλγεʼ', None),\n",
       " ('ἔθηκε', 'V3SAIA---'),\n",
       " ('πολλὰς', 'A-P---FA-'),\n",
       " ('δʼ', None),\n",
       " ('ἰφθίμους', 'A-P---MA-'),\n",
       " ('ψυχὰς', 'N-P---FA-'),\n",
       " ('Ἄϊδι', None),\n",
       " ('προΐαψεν', None),\n",
       " ('ἡρώων', 'N-P---MG-'),\n",
       " ('αὐτοὺς', 'A-P---MA-'),\n",
       " ('δὲ', 'G--------'),\n",
       " ('ἑλώρια', 'N-P---NA-'),\n",
       " ('τεῦχε', 'V3SIIA---'),\n",
       " ('κύνεσσιν', 'N-P---MD-')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_ngram_123_backoff(homerText[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prosody scanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.prosody.greek.scanner import Scansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner = Scansion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\cltk\\prosody\\greek\\scanner.py\", line 162, in _long_by_position\n",
      "    next_syll = sentence[sentence.index(syllable) + 1]\n",
      "IndexError: list index out of range\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\logging\\__init__.py\", line 1028, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode characters in position 96-97: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2877, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2922, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3146, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-120-3fb8b2cceef0>\", line 1, in <module>\n",
      "    scanner.scan_text('νέος μὲν καὶ ἄπειρος, δικῶν ἔγωγε ἔτι. μὲν καὶ ἄπειρος')\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\cltk\\prosody\\greek\\scanner.py\", line 264, in scan_text\n",
      "    meter = self._scansion(sentence_syllables)\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\cltk\\prosody\\greek\\scanner.py\", line 191, in _scansion\n",
      "    if self._long_by_position(syllable, sentence) or \\\n",
      "  File \"C:\\Users\\Slavist29\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\cltk\\prosody\\greek\\scanner.py\", line 177, in _long_by_position\n",
      "    logger.info(\"IndexError while checking if syllable '%s' is long. Continuing.\", syllable)\n",
      "Message: \"IndexError while checking if syllable '%s' is long. Continuing.\"\n",
      "Arguments: ('τι',)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['˘¯¯¯˘¯¯˘¯˘¯˘˘x']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scanner.scan_text('νέος μὲν καὶ ἄπειρος, δικῶν ἔγωγε ἔτι. μὲν καὶ ἄπειρος')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.greek.sentence import SentenceTokenizer\n",
    "sent_tokenizer = SentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['μῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος οὐλομένην ἣ μυρίʼ Ἀχαιοῖς ἄλγεʼ ἔθηκε πολλὰς δʼ ἰφθίμους ψυχὰς Ἄϊδι προΐαψεν ἡρώων αὐτοὺς δὲ ἑλώρια τεῦχε κύνεσσιν  ']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenizer.tokenize(homerText[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.sentence import TokenizeSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TokenizeSentence('greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens  = tokenizer.tokenize(homerText[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['μῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος οὐλομένην ἣ μυρίʼ Ἀχαιοῖς ἄλγεʼ ἔθηκε πολλὰς δʼ ἰφθίμους ψυχὰς Ἄϊδι προΐαψεν ἡρώων αὐτοὺς δὲ ἑλώρια τεῦχε κύνεσσιν  ']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.ir.query import search_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, 'v'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-2cbca5713676>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msearch_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'πνεῦμα'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tlg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sentence'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase_insensitive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_keyword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\cltk\\ir\\query.py\u001b[0m in \u001b[0;36msearch_corpus\u001b[1;34m(pattern, corpus, context, case_insensitive, expand_keyword, lemmatized, threshold)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mpunctuation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpattern\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[0msimilar_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_keyword_expander\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlemmatized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The following similar terms will be added to the '{0}' query: '{1}'.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilar_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\cltk\\ir\\query.py\u001b[0m in \u001b[0;36m_keyword_expander\u001b[1;34m(word, language, lemmatized, threshold)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimp_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m     \u001b[0msimilar_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlemmatized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msimilar_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\cltk\\vector\\word2vec.py\u001b[0m in \u001b[0;36mget_sims\u001b[1;34m(word, language, lemmatized, threshold)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir_abs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfnf_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfnf_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1328\u001b[0m         \"\"\"\n\u001b[0;32m   1329\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m             \u001b[1;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m         \"\"\"\n\u001b[1;32m-> 1244\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1245\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ns_exponent'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \"\"\"\n\u001b[1;32m--> 603\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loaded %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36munpickle\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m   1382\u001b[0m         \u001b[1;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, 'v'."
     ]
    }
   ],
   "source": [
    "for x in search_corpus('πνεῦμα', 'tlg', context='sentence', case_insensitive=True, expand_keyword=True, threshold=0.5):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
