{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries to DeepLearning\n",
    "\n",
    "In this notebook I'll try to extract informations about the context in which verbs are used in Homer. \n",
    "This will then build the database on which I can train a NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from nltk.tokenize import word_tokenize\n",
    "from more_itertools import locate\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(txt,lines = False):\n",
    "    '''\n",
    "    loads text from txt files\n",
    "    '''\n",
    "    if lines:\n",
    "        with open(txt, \"r\",encoding = \"UTF-8\") as source:\n",
    "            data = source.readlines()\n",
    "    else:\n",
    "        with open(txt, \"r\",encoding = \"UTF-8\") as source:\n",
    "            data = source.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = re.sub(r\"\\n\", \" \",txt, re.MULTILINE) # delete row break\n",
    "    txt = re.sub(r\"\\\\n\", \" \",txt, re.MULTILINE) # delete row break\n",
    "    txt = re.sub(\"n\", \"\",txt, re.MULTILINE) # delete row break\n",
    "    txt = re.sub(r\";\",\"?\",txt,re.MULTILINE) # change question mark\n",
    "    txt = re.sub(r\"(\\d+(\\.\\d+)?)\",\"\",txt,0,re.MULTILINE) # delete verse number\n",
    "    txt = re.sub(r\"\\w(·)\",\"\",txt,0,re.MULTILINE)\n",
    "    txt = re.sub(r'(\\.|,|;|!|\\\")',\"\",txt,re.MULTILINE)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_verbs(parsed_tokens):\n",
    "    '''\n",
    "    Given the from xml extracted tokens and parsing, this function extracts only the verbs.\n",
    "\n",
    "    return: \n",
    "        verbs : a list of lemmata\n",
    "    '''\n",
    "    verbs = []\n",
    "    for d in data:\n",
    "        infos = d.split(\"|\")\n",
    "        regex = r\"v.*\"\n",
    "        match = re.match(regex, infos[1])\n",
    "        if match is not None:\n",
    "            verbs.append(infos[2][:-1])\n",
    "    # clean duplicates and return\n",
    "    return list(set(verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homer_dictionary(xml_data):\n",
    "    '''\n",
    "    Given the xml parsed data, returns a long dictionary: v_form : pos_tagging\n",
    "    '''\n",
    "    ilias_dict = {}\n",
    "    for d in xml_data:\n",
    "        el = d.split('|')\n",
    "        if el[0] not in ilias_dict.keys():\n",
    "            ilias_dict[el[0]] = el[1]\n",
    "    return ilias_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(stopword_file,tokens):\n",
    "    with open(stopword_file,\"r\",encoding=\"UTF-8\") as stopw:\n",
    "        stopwords = stopw.read()\n",
    "\n",
    "    stopwords = stopwords.split('\\n')\n",
    "    stopwords.extend('.')\n",
    "    stopwords.extend(',')\n",
    "    stopwords.extend('“')\n",
    "    stopwords.extend('·')\n",
    "    print(f\"Len of tokens before cleaning: {len(tokens)}\")\n",
    "    data_clean = [w.lower() for w in tokens if w not in stopwords]\n",
    "    print(f\"Done!\\nLen of tokens after cleaning: {len(data_clean)}\")\n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allIndices(sent, wanted):\n",
    "    '''\n",
    "    Finds all indices of a wanted word in a given text\n",
    "    '''\n",
    "    indices = list(locate(sent, lambda a : a == wanted))\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbours(corpus,target_list,window=9):\n",
    "    '''\n",
    "    corpus = token list (as result of a tokenizer)\n",
    "    target_list = verbal forms for a given verb\n",
    "    window = window length\n",
    "\n",
    "    '''\n",
    "    context = []\n",
    "    # iterate per verb_form\n",
    "    for verb_form in list(target_list):\n",
    "        # iterate per sentence\n",
    "        indices = allIndices(corpus,verb_form)\n",
    "        for occurrence in indices:\n",
    "            for j in range(max(occurrence-window,0),min(occurrence+window, len(corpus))):\n",
    "                if j not in indices:\n",
    "                    context.append(corpus[j])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_neighbours(verb,corpus,window=8,print_common = True,hmany=100):\n",
    "    '''\n",
    "    Given a verb (infinitive) or Perseus quotation form,it finds all the paradigmatic form in the Text and returns the neighbours.\n",
    "\n",
    "    Return:\n",
    "\n",
    "    a tuple:\n",
    "    context : the neighbours (Counter dictionary)\n",
    "    verb_forms : the set of paradigmatic forms for the given verb\n",
    "    '''\n",
    "    verb_forms = set()\n",
    "    for d in data:\n",
    "        el = d.split('|')\n",
    "        if el[2] == verb+\"\\n\":\n",
    "            verb_forms.add(el[0])\n",
    "    context = Counter(find_neighbours(corpus,verb_forms,window = window))\n",
    "    if print_common:\n",
    "        pprint(context.most_common(hmany), compact=True)\n",
    "    return (context,verb_forms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer():\n",
    "    '''\n",
    "    This class takes a verb in the quotation form on Perseus and returns a full analysis of the grammatical contexts in which it appears in Homer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,verb):\n",
    "        # main form\n",
    "        self.verb = verb\n",
    "        \n",
    "        # counters for the neighbourhood along grammatical classes\n",
    "        self.pos = {'n' : 0, 'v':0,'a' : 0,'d' : 0,'l' : 0,'g' : 0,'c' : 0,'r' : 0,'p' : 0,'m' : 0,'m' : 0,'i' : 0,'u':0}\n",
    "        self.tense = {'p':0,'i':0,'l':0,'r':0,'t':0,'f':0,'a':0}\n",
    "        self.voice = {'a':0, 'p':0, 'm':0, 'e':0}\n",
    "        self.case = {'n':0,'g':0,'d':0,'a':0,'v':0,'l':0,}\n",
    "        self.mood = {'i':0,'s':0,'o':0,'n':0,'m':0,'p':0}\n",
    "\n",
    "        # Counters\n",
    "        self.context = None\n",
    "        \n",
    "        # total forms in the neighbourhood\n",
    "        self.context_words = 0\n",
    "\n",
    "        # set of the paradigmatic forms\n",
    "        self.v_forms = set()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        print(f\"Analyzer für das Verb {self.verb}\")\n",
    "        \n",
    "    def extract_context(self,corpus):\n",
    "        neighbours = extract_neighbours(self.verb,corpus = corpus,print_common=False)\n",
    "        self.context = neighbours[0]\n",
    "        self.v_forms = neighbours[1]\n",
    "    \n",
    "    def analyze(self,lexicon,corpus):\n",
    "        if self.context == None:\n",
    "            self.extract_context(corpus=corpus)\n",
    "        self.context_words = sum(self.context.values())\n",
    "        pos = []\n",
    "        tense = []\n",
    "        mood = []\n",
    "        voice = []\n",
    "        case = []\n",
    "        for w in list(self.context):\n",
    "            if w in lexicon.keys() and len(lexicon[w]) > 2 : # if the word is present and if this is not empty\n",
    "                morpho_info = lexicon[w]\n",
    "                pos.append(morpho_info[0]) #w[0] = pos\n",
    "                if morpho_info[3] != \"-\":\n",
    "                    tense.append(morpho_info[3]) #w[3] = tense/aspect\n",
    "                if morpho_info[4] != \"-\":\n",
    "                    mood.append(morpho_info[4]) #w[4] = mood\n",
    "                if morpho_info[5] != \"-\":\n",
    "                    voice.append(morpho_info[5]) #w[5] = voice\n",
    "                if morpho_info[7] != \"-\":\n",
    "                    case.append(morpho_info[7]) #w[7] = case\n",
    "\n",
    "        pos = Counter(pos)\n",
    "        mood = Counter(mood)\n",
    "        voice = Counter(voice)\n",
    "        tense = Counter(tense)\n",
    "        case = Counter(case)\n",
    "        \n",
    "        for x in pos.keys():\n",
    "            self.pos[x] = pos[x]\n",
    "        for x in mood.keys():\n",
    "            self.mood[x] = mood[x]\n",
    "        for x in tense.keys():\n",
    "            self.tense[x] = tense[x]\n",
    "        for x in voice.keys():\n",
    "            self.voice[x] = voice[x]\n",
    "        for x in case.keys():\n",
    "            self.case[x] = case[x]\n",
    "    \n",
    "    def normalize(self):\n",
    "        self.pos = self.get_pos()\n",
    "        self.mood = self.get_mood()\n",
    "        self.voice = self.get_voice()\n",
    "        self.tense = self.get_tense()\n",
    "        self.case = self.get_case()\n",
    "        \n",
    "        \n",
    "    def get_info(self,category,pr = False):\n",
    "        cat_c_normalized = {}\n",
    "        for w,c in category.items():\n",
    "            try:\n",
    "                cat_c_normalized[w] = round(c/self.context_words,5)\n",
    "            except:\n",
    "                cat_c_normalized[w] = 0.0\n",
    "            if pr:\n",
    "                print(f\"{w} : {cat_c_normalized[w]}\")\n",
    "        return cat_c_normalized\n",
    "            \n",
    "    def get_pos(self,pr=False):\n",
    "        return self.get_info(self.pos,pr)\n",
    "        \n",
    "    def get_tense(self,pr=False):\n",
    "        return self.get_info(self.tense,pr)\n",
    "        \n",
    "    def get_mood(self,pr=False):\n",
    "        return self.get_info(self.mood,pr)\n",
    "    \n",
    "    def get_voice(self,pr=False):\n",
    "        return self.get_info(self.voice,pr)\n",
    "    \n",
    "    def get_case(self,pr=False):\n",
    "        return self.get_info(self.case,pr)\n",
    "    \n",
    "    def get_all(self,pr=True):\n",
    "        print(\"POS:\\n\")\n",
    "        self.get_pos(pr=pr)\n",
    "        print(\"\\nTENSE\\n\")\n",
    "        self.get_tense(pr=pr)\n",
    "        print(\"\\nVOICE\\n\")\n",
    "        self.get_voice(pr=pr)\n",
    "        print(\"\\nMOOD\\n\")\n",
    "        self.get_mood(pr=pr)\n",
    "        print(\"\\nCASE\\n\")\n",
    "        self.get_case(pr=pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens annotiert\n",
    "data = load(\"homerische_tokens.txt\",lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw text (Ilias + Odyssee)\n",
    "homer = load(\"HomerGesamt.txt\")\n",
    "# clean it\n",
    "homer = clean_text(homer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'μῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος οὐλομένην ἣ μυρίʼ Ἀχαιοῖς ἄλγεʼ ἔθηκε πολλὰς δʼ ἰφθίμους ψυχὰς Ἄϊδι προΐαψεν ἡρώων αὐτοὺς δὲ ἑλώρια τεῦχε κύνεσσιν  οἰωνοῖσί τε πᾶσι Διὸς δʼ ἐτελείετο βουλή ἐξ οὗ δὴ τὰ πρῶτα διαστήτην ἐρίσαντε Ἀτρεΐδης τε ἄναξ ἀνδρῶν καὶ δῖος Ἀχιλλεύς  τίς τʼ ἄρ σφωε θεῶν ἔριδι ξυνέηκε μάχεσθαι?\\nΛητοῦς καὶ Διὸς υἱό ὃ γὰρ βασιλῆϊ χολωθεὶς\\n  νοῦσον ἀνὰ στρατὸν ὄρσε κακήν ὀλέκοντο δὲ λαοί\\nοὕνεκα τὸν Χρύσην ἠτίμασεν ἀρητῆρα\\nἈτρεΐδη ὃ γὰρ ἦλθε θοὰς ἐπὶ νῆας Ἀχαιῶν\\nλυσόμενός τε θύγατρα φέρων τʼ ἀπερείσιʼ ἄποινα,\\nστέμματʼ ἔχων ἐν χερσὶν ἑκηβόλου Ἀπόλλωνος\\n  χρυσέῳ ἀνὰ σκήπτρῳ, καὶ λίσσετο πάντας Ἀχαιούς,\\nἈτρεΐδα δὲ μάλιστα δύω, κοσμήτορε λαῶ\\nἈτρεΐδαι τε καὶ ἄλλοι ἐϋκνήμιδες Ἀχαιοί,\\nὑμῖν μὲν θεοὶ δοῖεν Ὀλύμπια δώματʼ ἔχοντες\\nἐκπέρσαι Πριάμοιο πόλιν, εὖ δʼ οἴκαδʼ ἱκέσθα\\n  παῖδα δʼ ἐμοὶ λύσαιτε φίλην, τὰ δʼ ἄποινα δέχεσθαι,\\nἁζόμενοι Διὸς υἱὸν ἑκηβόλον Ἀπόλλωνα.\\n\\nἔνθʼ ἄλλοι μὲν πάντες ἐπευφήμησαν Ἀχαιοὶ\\nαἰδεῖσθαί θʼ ἱερῆα καὶ ἀγλαὰ δέχθαι ἄποιν\\nἀλλʼ οὐκ Ἀτρεΐδῃ Ἀγαμέμνονι ἥνδανε'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homer.replace(r'\\\\n',\" \")\n",
    "homer.replace(r'\\n', \" \")\n",
    "homer[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize homer gesamt\n",
    "homer_tokens = word_tokenize(homer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of tokens before cleaning: 553731\n",
      "Done!\n",
      "Len of tokens after cleaning: 421398\n"
     ]
    }
   ],
   "source": [
    "# stopwords delete\n",
    "tokens = delete_stopwords(\"stopwords.txt\",homer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping token --> pos\n",
    "homer_dict = homer_dictionary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v3saia---'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test dictionary\n",
    "homer_dict['ἔθηκε']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n-p---na-'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homer_dict['ἄποινα']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ὣς', 66), ('εἶναι', 57), ('ἔφατʼ', 43), ('ἀλλʼ', 28), ('αὐτὰρ', 28),\n",
      " ('ἐν', 26), ('γὰρ', 25), ('ἐπὶ', 25), ('νῦν', 24), ('ἔπος', 23), ('ἔκλυε', 23),\n",
      " ('”', 23), ('ῥʼ', 22), ('ἐκ', 22), ('ἐγὼ', 22), ('διὶ', 22), ('δὴ', 20),\n",
      " ('διὸς', 20), ('γένος', 20), ('ηὔδ', 19), ('οὔ', 18), ('γʼ', 18), ('γε', 18),\n",
      " ('ἀθήνη', 18), ('ποσειδάωνι', 18), ('ἔπειτα', 18), ('ἦ', 16), ('ἀχαιῶν', 16),\n",
      " ('ἀπόλλων', 16), ('ἐπεὶ', 16), ('ἐνὶ', 15), ('ἄρʼ', 15), ('ὃς', 15),\n",
      " ('παλλὰς', 15), ('κλῦθι', 14), ('ἐξ', 14), ('μάλʼ', 14), ('ἠδʼ', 13),\n",
      " ('ὡς', 13), ('εἴ', 13), (';', 13), ('υἱὸς', 13), ('μέγα', 12), ('φοῖβος', 12),\n",
      " ('τις', 12), ('ἐπεί', 12), ('αὐτίκα', 12), ('κούρῃ', 12), ('οὐ', 12),\n",
      " ('χεῖρας', 12), ('οὐλοχύτας', 11), ('ζεῦ', 11), ('ἀγορεύσω', 11),\n",
      " ('ἄνακτ', 11), ('πατρὸς', 11), ('πολλὰ', 11), ('καί', 11), ('αἶψα', 11),\n",
      " ('μάλα', 10), ('οὕτω', 10), ('μοι', 10), ('μέν', 10), ('πατρὶ', 10),\n",
      " ('αἴ', 10), ('ἀνασχώ', 10), ('οὐδʼ', 9), ('θεοῖσιν', 9), ('»', 9),\n",
      " ('γαιήοχε', 9), ('ἐμὸς', 9), ('κατὰ', 9), ('υἱός', 9), ('εἶνα', 9),\n",
      " ('ἄλλοι', 9), ('ἱκάνω', 9), ('εἶναι.', 9), ('εἰς', 9), ('θεὸς', 8), ('ἄρα', 8),\n",
      " ('τίς', 8), ('ἀ', 8), ('οἴκαδʼ', 8), ('πρὸς', 8), ('κρονίωνι', 8),\n",
      " ('ἄνακτι', 8), ('κέ', 8), ('ἔμμεναι', 8), ('ἐγώ', 8), ('ῥέξειν', 8), ('σʼ', 8),\n",
      " ('αὖτε', 8), ('μάλιστα', 7), ('πᾶσι', 7), ('ἄναξ', 7), ('τ', 7),\n",
      " ('γερήνιος', 7), ('φάτο', 7), ('ὅτε', 7), ('μʼ', 7), ('σύ', 7)]\n"
     ]
    }
   ],
   "source": [
    "# test extraction\n",
    "euhomai_neighbours, euhomai_forms = extract_neighbours(\"εὔχομαι\",corpus=tokens)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'εὐξάμενοι', 'εὐξάμενος', 'εὐξάμενός', 'εὐξαίμην', 'εὐξαμένοιο', 'εὐξαμένου',\n",
      " 'εὐξαμένῃ', 'εὐχομένη', 'εὐχομένης', 'εὐχομένοιο', 'εὐχομένοισι', 'εὐχομένου',\n",
      " 'εὐχομένω', 'εὐχόμεθ̓', 'εὐχόμεναι', 'εὐχόμενοι', 'εὐχόμενος', 'εὐχόμενόν',\n",
      " 'εὔξαντο', 'εὔξατο', 'εὔξεαι', 'εὔχἐ', 'εὔχεαι', 'εὔχεο', 'εὔχεσθαι',\n",
      " 'εὔχεσθε', 'εὔχετ̓', 'εὔχεται', 'εὔχετο', 'εὔχομ̓', 'εὔχομαι', 'εὔχοντο'}\n"
     ]
    }
   ],
   "source": [
    "pprint(euhomai_forms, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "euhomai = Analyzer('εὔχομαι')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "euhomai.analyze(lexicon = homer_dict,corpus = tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "euhomai.extract_context(corpus=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4541"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of neighbours\n",
    "euhomai.context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS:\n",
      "\n",
      "n : 0.05616\n",
      "v : 0.06342\n",
      "a : 0.04096\n",
      "d : 0.01409\n",
      "l : 0.0\n",
      "g : 0.00529\n",
      "c : 0.0044\n",
      "r : 0.00573\n",
      "p : 0.00881\n",
      "m : 0.00066\n",
      "i : 0.00022\n",
      "u : 0.00022\n",
      "x : 0.00022\n",
      "\n",
      "TENSE\n",
      "\n",
      "p : 0.01586\n",
      "i : 0.00969\n",
      "l : 0.00088\n",
      "r : 0.00308\n",
      "t : 0.00022\n",
      "f : 0.00242\n",
      "a : 0.03127\n",
      "\n",
      "VOICE\n",
      "\n",
      "a : 0.04713\n",
      "p : 0.00088\n",
      "m : 0.00617\n",
      "e : 0.00903\n",
      "\n",
      "MOOD\n",
      "\n",
      "i : 0.03479\n",
      "s : 0.00308\n",
      "o : 0.00132\n",
      "n : 0.00749\n",
      "m : 0.00484\n",
      "p : 0.01189\n",
      "\n",
      "CASE\n",
      "\n",
      "n : 0.03656\n",
      "g : 0.02092\n",
      "d : 0.02158\n",
      "a : 0.03523\n",
      "v : 0.00352\n",
      "l : 0.0\n"
     ]
    }
   ],
   "source": [
    "euhomai.get_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeric_verbs = extract_verbs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random test\n",
    "'φαίνω' in homeric_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"homerische_daten.txt\",\"a\",encoding=\"UTF-8\") as daten:\n",
    "        daten.write('verb|occurrences|noun|verb|adjective|adverb|article|particle|conjunction|preposition|pronoun|numeral|interjection|punctuation|present|imperfect|pluperfect|perfect|futPerfect|future|aorist|indicative|subjunctive|optative|infinitive|imperative|participle|active|passive|middle|mediopassive|nom|gen|dat|acc|voc|loc|class\\n')\n",
    "        for v in homeric_verbs:\n",
    "            v_analyzer = Analyzer(v)\n",
    "            v_analyzer.analyze(corpus=tokens, lexicon=homer_dict)            \n",
    "            v_analyzer.normalize()\n",
    "            # counts\n",
    "            count = 0\n",
    "            for f in v_analyzer.v_forms:\n",
    "                count += len(allIndices(tokens,f))\n",
    "            # first infos: verb_lemma + count of occurrences of the paradigmatic form in text\n",
    "            daten.write(f\"{v}|{count}|\")\n",
    "            \n",
    "            # pos \n",
    "            daten.write(str(v_analyzer.pos['n']) + \"|\" + str(v_analyzer.pos['v'])+ \"|\" +str(v_analyzer.pos['a'])+ \"|\" +str(v_analyzer.pos['d'])+ \"|\" +str(v_analyzer.pos['l'])+ \"|\" +str(v_analyzer.pos['g'])+ \"|\" +str(v_analyzer.pos['c'])+ \"|\" +str(v_analyzer.pos['r'])+ \"|\" +str(v_analyzer.pos['p'])+ \"|\" +str(v_analyzer.pos['m'])+ \"|\" +str(v_analyzer.pos['i'])+ \"|\" +str(v_analyzer.pos['u']) + \"|\")\n",
    "            \n",
    "            # tenses and aspects\n",
    "            daten.write(str(v_analyzer.tense['p']) +\"|\"+str(v_analyzer.tense['i']) +\"|\"+str(v_analyzer.tense['l']) +\"|\"+str(v_analyzer.tense['r']) +\"|\"+str(v_analyzer.tense['t']) +\"|\"+str(v_analyzer.tense['f']) +\"|\"+str(v_analyzer.tense['a']) +\"|\")\n",
    "            \n",
    "            # moods\n",
    "            daten.write(str(v_analyzer.mood['i']) +\"|\"+str(v_analyzer.mood['s']) +\"|\"+str(v_analyzer.mood['o']) +\"|\"+str(v_analyzer.mood['n']) +\"|\"+str(v_analyzer.mood['m']) +\"|\"+str(v_analyzer.mood['p']) +\"|\")\n",
    "            \n",
    "            # voice\n",
    "            daten.write(str(v_analyzer.voice['a']) +\"|\"+str(v_analyzer.voice['p']) +\"|\"+str(v_analyzer.voice['m']) +\"|\"+str(v_analyzer.voice['e']) +\"|\")\n",
    "\n",
    "            # case\n",
    "            daten.write(str(v_analyzer.case['n'])+\"|\"+str(v_analyzer.case['g'])+\"|\"+str(v_analyzer.case['d'])+\"|\"+str(v_analyzer.case['a'])+\"|\"+str(v_analyzer.case['v'])+\"|\"+str(v_analyzer.case['l']))\n",
    "            # \n",
    "            daten.write(\"|\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
